---
title: "Predicting Self-Rated Mental Health Based on Demographic and Family Traits"
author: "James Bao, Alan Chen, Xinyi Zhang, Rose"
date: "10/19/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract
Predicting mental health in the context of the nuclear family. 
Does being married have a positive or negative impact?

# Introduction

# Data
The dataset we used in our modeling is the 2017 General Social Survey (Family cycle). The following sections will discuss how the data was collected, what the key features of the dataset are, and what the data looks like.

## Data Collection
From February 1, 2017 to November 30, 2017, Statistics Canada gathered data on the Canadian family unit by conducting voluntary telephone interviews. Their target population was all non-institutionalized individuals living in Canada, aged 15 or older. Cross-sectional sampling was conducted in a two-stage design. 

The stratified simple random sampling method was used in the first stage. Here, the sampling frame consisted of telephone numbers from the Census grouped as households using data from Statistic Canada's dwelling frame. Strata were formed at the census metropolitan area (CMA) level and at the province level (i.e., large CMAs formed their own strata, smaller CMAs were grouped together, and the non-CMA regions of each province were grouped together), forming a total of 27 non-overlapping strata. Finally, households were sampled randomly from each strata such that the number sampled units from each strata corresponded to the population sizes of each strata. To reiterate, the sampled population for this first stage was the chosen households from each strata. 

The stratified simple random sampling method was also used in the second stage. Here, the sampling frame was a list of household members, aged 15 and older, from the households selected in the first stage. Then, one individual was randomly selected from each household, forming the sampled population. Approximately 43,000 individuals were contacted to participate in the survey.

Overall, the surveying method using two-stage simple random stratified sampling is effective in generating a sample that geographically representative of the Canadian population. In addition to estimates about the Canadian population at large, the stratified sampling method also allows estimates to be made about subpopulations (at the province level). 

Statistics Canada reported that the non-response rate was 47.6%. To reduce the effects of non-response bias, survey responses were adjusted based on the demographic characteristics of households that were non-responsive (by pulling their information from the 2016 Census). This ensures that the discrepancy between the target population and survey responses resulting from non-response is minimized. Furthermore, for the Family cycle of the GSS, responses were also adjusted for income and household size to make more accurate survey estimates for the variables of interest.

Statistics Canada did not disclose the true cost of conducting the survey but we can make some speculations based on the available information about their field work methodology. Surveying was conducted using Computer Assisted Telephone Interviewing (CATI) wherein interviewers read aloud the computerized questionnaire and immediately record the respondent's answers. Although this allows for a reduction in costs compared to traditional in-person surveying, labor costs still include time spent computerizing the survey, training interviewers, and having interviewers administer the questionnaire. Other labor costs include designing the questionnaire and surveying methodology as well as conducting quality control (data consistency was checked by the CATI system during surveying and unresolved inconsistencies were handled afterwards by support staff). Non-labor costs likely included paying for equipment, phone service, offices, and so forth. Again, although we don't have exact costs, we can conclude that the time and costs associated with conducting the GSS is a clear reason why it is only administered once a year.

Per the report on the 2017 GSS from Statistics Canada, extensive research and testing was conducted when designing the questionnaire. Consequently, a major strength of the questionnaire is that it contains focused questions that comprehensively and extensively capture the subject of interest (the Canadian family). Upon reading through the questionnaire made available by Statistics Canada, the wording of each question is precise and clear, leaving little room for ambiguity. Additionally, another strength of the survey is that a vast majority of questions were objective (dates, events, counts) removing potential response biases that occur with subjective questions. (Not all questions were objective however, in fact the variable of interest we will model in subsequent sections consists of subjective responses.) On the other hand, because of the specificity of the questions, the survey is very long with several dozens sections and several questions per section. Furthermore, as a result of the large scope of the target population, many questions in the survey did not apply to a large majority of respondents (e.g., number of grandchildren, questions about additional marriages, etc.). The data collected is also incomplete because participants were given the option to refuse to answer or answer "I don't know" to each question since participation was voluntary.

Overall, the surveying methodology and distributed questionnaire were carefully designed in the interest of collecting accurate, representative data wherever possible.

## Data Characteristics
The full dataset of responses to the 2017 General Social Survey (Family cycle) contains 20,602 observations for over 400 variables relating to the Canadian family. A large reason for our choice to use this dataset is because it is the most recent GSS cycle available for modeling. Other benefits of this dataset have been previously touched upon in the previous section. Namely, the data was checked for consistency in real time by the CATI system (as well as by survey support staff) so there is a certain measure of accuracy that other survey results lack. Additionally, the stratified simple random sampling method used to distribute the survey suggests that the results are representative of the Canadian population to some degree (in the geographical sense at the very least). A major weakness of the data is that it is not complete because of the voluntary nature of the surveying.

In the interest of space, we will only discuss the variables in the dataset that are relevant to our model. The variable we aim to predict is self_rated_mental_health while the factors that we chose to inform this prediction are age, sex, marital_status, and self_rated_health. We chose these factors based on the demographic information mentioned in mental health statistics (age, sex, and health) and based on what we suspected might contribute to mental health in the context of family composition (marital status). More explicitly, here is what information the chosen variables in the dataset represent:

* age (agedc in the original dataset): the exact age of the respondent (in decimals) at the time of the survey
* sex (sex): sex of the respondent, the options being "Male" or "Female"
* marital_status (marstat): marital status of the respondent, the options being "Single, never married", "Married", "Living common-law", "Separated" (but still legally married), "Divorced", or "Widowed"
* self_rated_health (srh_110): self-rated health, the options being "Excellent", "Very good", "Good", "Fair", and "Poor"
* self_rated_mental_health (srh_115): self-rated mental health, the options being "Excellent", "Very good", "Good", "Fair", and "Poor"


# Model

Bayes' Theorem for Naive Bayes Classifier:
$P(c|x)=\frac{P(x|c)P(c)}{P(x)}$


```{r eval=F}
install.packages('tidyverse')
install.packages("sjstats")
install.packages("ROCR")
install.packages("brms")
install.packages("modelr")
install.packages("nnet")
install.packages("tidybayes")
require(nnet)
library(tidyverse) # for data manipulation and plots
```
```{r}

# using 12 rows of dummy data until we get the real data

data <- tibble(
  sex = c("M", "F", "M", "M", "F", "F", "F" ,"M", "F", "M", "F", "M"),
  martial_status = c("Married", "Divorced", "Single, never married", "Living common-law", "Widowed", "Separated", "Married", "Divorced", "Single, never married", "Married", "Divorced", "Single, never married"),
  age = c("under 18", "18-34", "35-50", "50-70", "over 70", "under 18", "18-34", "35-50", "under 18", "18-34", "35-50", "18-34"),
  education = c("Bachelor's", "High school", "College", "Trade Cert", "University degree above Bachelor's level", "University degree below Bachelor's level", "Bachelor's", "High school", "College", "Trade Cert", "Bachelor's", "High school"),
  has_regilious_afflication = c("Y", "Y", "N", "Y", "Y", "N", "Y", "Y", "N", "Y", "Y", "N"),
  has_children = c("Y", "N", "Y", "N", "Y", "N", "Y", "N", "Y", "N", "N", "N"),
  selfrated_mental_health = c("Fair", "Good", "Very Good", "Excellent", "Poor", "Fair", "Good", "Very Good", "Good", "Very Good", "Excellent", "Good")
)
data


# add 1 after all x-y combinations to avoid zero frequency problem

# count y
poor <- data %>% filter(selfrated_mental_health=="Poor") %>% tally() + 1
fair <- data %>% filter(selfrated_mental_health=="Fair") %>% tally() + 1
good <- data %>% filter(selfrated_mental_health=="Good") %>% tally() + 1
vgood <- data %>% filter(selfrated_mental_health=="Very Good") %>% tally() + 1
excellent <- data %>% filter(selfrated_mental_health=="Excellent") %>% tally() + 1

total_mental <- poor + fair + good + vgood + excellent

# count sex given y
male_poor <- data %>% filter(sex=="M" & selfrated_mental_health=="Poor") %>% tally() + 1
male_fair <- data %>% filter(sex=="M" & selfrated_mental_health=="Fair") %>% tally() + 1
male_good <- data %>% filter(sex=="M" & selfrated_mental_health=="Good") %>% tally() + 1
male_vgood <- data %>% filter(sex=="M" & selfrated_mental_health=="Very Good") %>% tally() + 1
male_excellent <- data %>% filter(sex=="M" & selfrated_mental_health=="Excellent") %>% tally() + 1
female_poor <- data %>% filter(sex=="F" & selfrated_mental_health=="Poor") %>% tally() + 1
female_fair <- data %>% filter(sex=="F" & selfrated_mental_health=="Fair") %>% tally() + 1
female_good <- data %>% filter(sex=="F" & selfrated_mental_health=="Good") %>% tally() + 1
female_vgood <- data %>% filter(sex=="F" & selfrated_mental_health=="Very Good") %>% tally() + 1
female_excellent <- data %>% filter(sex=="F" & selfrated_mental_health=="Excellent") %>% tally() + 1

total_male <- male_poor + male_fair + male_good + male_vgood + male_excellent
total_female <- female_poor + female_fair + female_good + female_vgood + female_excellent

# count has_children given y
has_children_poor <- data %>% filter(has_children=="Y" & selfrated_mental_health=="Poor") %>% tally() + 1
has_children_fair <- data %>% filter(has_children=="Y" & selfrated_mental_health=="Fair") %>% tally() + 1
has_children_good <- data %>% filter(has_children=="Y" & selfrated_mental_health=="Good") %>% tally() + 1
has_children_vgood <- data %>% filter(has_children=="Y" & selfrated_mental_health=="Very Good") %>% tally() + 1
has_children_excellent <- data %>% filter(has_children=="Y" & selfrated_mental_health=="Excellent") %>% tally() + 1
no_children_poor <- data %>% filter(has_children=="N" & selfrated_mental_health=="Poor") %>% tally() + 1
no_children_fair <- data %>% filter(has_children=="N" & selfrated_mental_health=="Fair") %>% tally() + 1
no_children_good <- data %>% filter(has_children=="N" & selfrated_mental_health=="Good") %>% tally() + 1
no_children_vgood <- data %>% filter(has_children=="N" & selfrated_mental_health=="Very Good") %>% tally() + 1
no_children_excellent <- data %>% filter(has_children=="N" & selfrated_mental_health=="Excellent") %>% tally() + 1

total_has_children <- has_children_poor + has_children_fair + has_children_good + has_children_vgood + has_children_excellent
total_no_children <- no_children_poor + no_children_fair + no_children_good + no_children_vgood + no_children_excellent

# repeat for the rest of the independent var



## example: predict mental health state given male, has children
# calculating prob(poor)
class_prior_prob_poor <- poor / total_mental
likelihood_poor <- (male_poor/poor) * (has_children_poor/poor)
predictor_prior_prob_poor <- (total_male/total_mental) * (total_has_children/total_mental)
prob_poor <- likelihood_poor * class_prior_prob_poor / predictor_prior_prob_poor

prob_poor

# repeat this process to get probability for fair, good, vgood, excellent
# the classification will be the class with the largest probability

# note that the probability value themselves should not be taken seriously since naive bayes is a bad estimator (so we cannot say how confident we are in this classification)





```


```{r}
# load the csv, can be downloaded via utoronto
poll <- as_tibble(data.frame(read_csv("gss_cleaned.csv"))) 

# choose pertinent variables
poll <- poll %>% select(age, sex, marital_status,self_rated_health, 
                        self_rated_mental_health)

# clean up the data
poll$self_rated_mental_health %>% table()
poll<-poll[!grepl("Don't know", poll$self_rated_mental_health),]
poll$self_rated_mental_health %>% table()

# poll <- head(poll, 1000)

```
```{r}


model <- nnet::multinom(self_rated_mental_health ~ age + sex + marital_status + self_rated_health, 
                  data = poll)
summary(model)

```

```{r}
head(fitted(model))

input <- data.frame(self_rated_health = c("Excellent"), age = c(21.5), sex = c("Male"), marital_status = c("Single, never married"))
predict(model, newdata = input, "probs")
```

The purpose of the model is to predict a person's self-rated mental health based on other characteristics presented in the dataset such as demographics, lifestyle, socioeconomic status, etc. Since the self-rated mental health data is represented in categories, the task at its core is a classification problem.

Some models that were considered were linear regression, Naive Bayes, binary logistic regression, and multinomial logistic regression. To begin, linear regression is not suitable because it is often difficult to find an accurate linear relationship between predictors and categories, not to mention the fact that linear regression is more suitable when the dependent variable is continuous. Naive Bayes would be a viable option since it is able to handle classification of more than two categories using joint probability and Bayes' Theorem. However, Naive Bayes only works well under the assumption that the explanatory variables are independent, but this is often not the case. Given the context of the data, it is highly anticipated that the characteristics of a person are correlated in some way or another.

In addition, generative models (eg. Naive Bayes) has a higher asymptotic error than discriminative models (eg. logistic regression), but it approaches the asymptotic error faster. In other words, discriminative models tend to perform better given large enough data and generative models will perform better on less data as it learns faster. Since the dataset is large, choosing a discriminative model would be more appropriate for this task.

It is also important to note that the dependent variable has more than two categories. One way to handle this is to group multiple categories together so that there are only two categories which can be used for binary logistic regression. While this simplifies the complexity of implementing the model itself, there will be a loss in information from merging classes and it alters the original research task. Another option to handle multi-classification is to use multinomial logistic regression which is extended from binary logistic regression. The basic idea of this approach is, given k classes, create k binary logistic regression models (one for each class). The i-th binary model (1<=i<=k) predicts the probability that the given predictors lead to class i or not (one-versus-rest). The binary class with the highest probability will be the output prediction of the multinomial model. One of the caveats of this approach is that the even more data is required to provide enough for all binary logistic regression models for each class; otherwise, this method is prone to overfitting. This is less of a concern in this case given the size of the dataset and having 5~ classes to predict. Thus, multinomial logistic regression is the chosen model for this task.

<!--TODO: talk about selecting indepedant variables (as well as any pre-processing done on them before feeding into the model and why -- eg. age number vs age group), software used to run the model, model diagonstics/checks/convergence??? -->

# Results
# Discussion
## Weaknesses
## Next Steps

# Appendix

# References
Interview method/survey size: https://www.statcan.gc.ca/eng/survey/household/4501
Detailed information about GSS 2017: https://www23.statcan.gc.ca/imdb/p2SV.pl?Function=getSurvey&Id=335816
Questionnaire: https://www23.statcan.gc.ca/imdb/p3Instr.pl?Function=assembleInstr&lang=en&Item_Id=335815#qb345205
Mental health statistics: https://www.camh.ca/en/Driving-Change/The-Crisis-is-Real/Mental-Health-Statistics
Discriminative vs Generative Classifiers (Naive Bayes vs lostic regression): https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf